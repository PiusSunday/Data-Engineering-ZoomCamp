{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52bad16a",
   "metadata": {},
   "source": [
    "# Data loading \n",
    "\n",
    "Here we will be using the ```.paraquet``` file we downloaded and do the following:\n",
    " - Check metadata and table datatypes of the paraquet file/table\n",
    " - Convert the paraquet file to pandas dataframe and check the datatypes. Additionally check the data dictionary to make sure you have the right datatypes in pandas, as pandas will automatically create the table in our database.\n",
    " - Generate the DDL CREATE statement from pandas for a sanity check.\n",
    " - Create a connection to our database using SQLAlchemy\n",
    " - Convert our huge paraquet file into a iterable that has batches of 100,000 rows and load it into our database."
   ]
  },
  {
   "cell_type": "code",
   "id": "afef2456",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from time import time"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c750d1d4",
   "metadata": {},
   "source": [
    "# Read metadata \n",
    "pq.read_metadata('yellow_tripdata_2023-09.parquet')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a970fcf0",
   "metadata": {},
   "source": [
    "# Read file, read the table from file and check schema\n",
    "file = pq.ParquetFile('yellow_tripdata_2023-09.parquet')\n",
    "table = file.read()\n",
    "table.schema"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "43f6ea7e",
   "metadata": {},
   "source": [
    "# Convert to pandas and check data \n",
    "df = table.to_pandas()\n",
    "df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ccf039a0",
   "metadata": {},
   "source": [
    "We need to first create the connection to our postgres database. We can feed the connection information to generate the CREATE SQL query for the specific server. SQLAlchemy supports a variety of servers."
   ]
  },
  {
   "cell_type": "code",
   "id": "44e701ae",
   "metadata": {},
   "source": [
    "# Create an open SQL database connection object or a SQLAlchemy connectable\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine('postgresql://root:root@localhost:5432/nyc_tlc_trd')\n",
    "engine.connect()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c96a1075",
   "metadata": {},
   "source": [
    "# Generate CREATE SQL statement from schema for validation\n",
    "print(pd.io.sql.get_schema(df, name = 'nyc_tlc_data', con = engine))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eca7f32d",
   "metadata": {},
   "source": [
    "Datatypes for the table looks good! Since we used paraquet file the datasets seem to have been preserved. You may have to convert some datatypes so it is always good to do this check."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a751ed",
   "metadata": {},
   "source": [
    "## Finally inserting data\n",
    "\n",
    "There are 2,846,722 rows in our dataset. We are going to use the ```parquet_file.iter_batches()``` function to create batches of 100,000, convert them into pandas and then load it into the postgres database."
   ]
  },
  {
   "cell_type": "code",
   "id": "e20cec73",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#This part is for testing\n",
    "\n",
    "\n",
    "# Creating batches of 100,000 for the paraquet file\n",
    "batches_iter = file.iter_batches(batch_size = 100000)\n",
    "batches_iter\n",
    "\n",
    "# Take the first batch for testing\n",
    "df = next(batches_iter).to_pandas()\n",
    "df\n",
    "\n",
    "# Creating just the table in postgres\n",
    "#df.head(0).to_sql(name='ny_taxi_data',con=engine, if_exists='replace')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7fdda025",
   "metadata": {},
   "source": [
    "# Insert values into the table \n",
    "t_start = time()\n",
    "count = 0\n",
    "for batch in file.iter_batches(batch_size = 100000):\n",
    "    count += 1\n",
    "    batch_df = batch.to_pandas()\n",
    "    print(f'inserting batch {count}...')\n",
    "    b_start = time()\n",
    "\n",
    "    batch_df.to_sql(name = 'nyc_tlc_data', con = engine, if_exists = 'append')\n",
    "    b_end = time()\n",
    "    print(f'inserted! time taken {b_end - b_start:10.3f} seconds.\\n')\n",
    "\n",
    "t_end = time()\n",
    "print(f'Completed! Total time taken was {t_end - t_start:10.3f} seconds for {count} batches.')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a7c102be",
   "metadata": {},
   "source": [
    "## Extra bit\n",
    "\n",
    "While trying to do the SQL Refresher, there was a need to add a lookup zones table but the file is in ```.csv``` format. \n",
    "\n",
    "Let's code to handle both ```.csv``` and ```.paraquet``` files!"
   ]
  },
  {
   "cell_type": "code",
   "id": "a643d171",
   "metadata": {},
   "source": [
    "from time import time\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from sqlalchemy import create_engine"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "62c9040a",
   "metadata": {},
   "source": [
    "url = 'https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv'\n",
    "url = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-09.parquet'\n",
    "\n",
    "file_name = url.rsplit('/', 1)[-1].strip()\n",
    "file_name"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e495fa96",
   "metadata": {},
   "source": [
    "import sys\n",
    "\n",
    "if '.csv' in file_name:\n",
    "    print('yay')\n",
    "    df = pd.read_csv(file_name, nrows = 10)\n",
    "    df_iter = pd.read_csv(file_name, iterator = True, chunksize = 100000)\n",
    "elif '.parquet' in file_name:\n",
    "    print('oh yea')\n",
    "    file = pq.ParquetFile(file_name)\n",
    "    df = next(file.iter_batches(batch_size = 10)).to_pandas()\n",
    "    df_iter = file.iter_batches(batch_size = 100000)\n",
    "else:\n",
    "    print('Error. Only .csv or .parquet files allowed.')\n",
    "    sys.exit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7556748f",
   "metadata": {},
   "source": [
    "This code is a rough code and seems to be working. The cleaned up version will be in `data-loading-parquet.py` file."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
